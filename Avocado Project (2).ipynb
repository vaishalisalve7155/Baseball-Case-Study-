{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56032633",
   "metadata": {},
   "source": [
    "# Avocado Dataset Analysis and ML Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285fa951",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e6d4d5c0",
   "metadata": {},
   "source": [
    ".  Problem Statement\n",
    ".  Data Loading and Description\n",
    ".  Data Profiling\n",
    "      .  Understanding the Dataset\n",
    "      .  Profiling\n",
    "      .  Preprocessing\n",
    ".  Data Visualisation and Questions answered\n",
    "       .  Q.1 Which type of Avocados are more in demand (Conventional or Organic)?\n",
    "       .  Q.2 In which range Average price lies, what is distribution look like?\n",
    "       .  Q.3 How Average price is distributed over the months for Conventional and Organic Types?\n",
    "       .  Q.4 What are TOP 5 regions where Average price are very high?\n",
    "       .  Q.5 What are TOP 5 regions where Average consumption is very high?\n",
    "       .  Q.6 In which year and for which region was the Average price the highest?\n",
    "       .  Q.7 How price is distributed over the date column?\n",
    "       .  Q.8 How dataset features are correlated with each other?\n",
    ".  Feature Engineering for Model building\n",
    ".  Model selection/predictions\n",
    "       .  P.1 Are we good with Linear Regression? Lets find out.\n",
    "       .  P.2 Are we good with Decision Tree Regression? Lets find out.\n",
    "       .  P.3 Are we good with Random Forest Regressor? Lets find out.\n",
    "       .  Lets see final Actual Vs Predicted sample.\n",
    ". Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ea5356",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3bd4c654",
   "metadata": {},
   "source": [
    ".  The notebooks explores the basic use of Pandas and will cover the basic commands of (EDA) for analysis purpose.\n",
    "\n",
    ".  In this study, we will try to see if we can predict the Avocado’s Average Price based on different features. The features are    different (Total Bags,Date,Type,Year,Region…).\n",
    "\n",
    ".  The variables of the dataset are the following:\n",
    ".  Categorical: ‘region’,’type’\n",
    ".  Date: ‘Date’\n",
    ".  Numerical:’Total Volume’, ‘4046’, ‘4225’, ‘4770’, ‘Total Bags’, ‘Small Bags’,’Large Bags’,’XLarge Bags’,’Year’\n",
    ".  Target:‘AveragePrice’\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c320aa0e",
   "metadata": {},
   "source": [
    "# Data Loading and Description"
   ]
  },
  {
   "cell_type": "raw",
   "id": "866e36af",
   "metadata": {},
   "source": [
    ".  This data was downloaded and provided by INSAID, from the Hass Avocado Board website in May of 2018 & compiled into a single    CSV.\n",
    "\n",
    ".  Represents weekly 2018 retail scan data for National retail volume (units) and price.\n",
    "\n",
    ".  The dataset comprises of 18249 observations of 14 columns. Below is a table showing names of all the columns and their          description.\n",
    "\n",
    ".  The unclear numerical variables terminology is explained in the next section:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "13574d05",
   "metadata": {},
   "source": [
    "           Features            Description\n",
    "          ‘Unamed: 0’         Its just a useless index feature that will be removed later\n",
    "          ‘Total Volume’      Total sales volume of avocados\n",
    "          ‘4046’              Total sales volume of Small/Medium Hass Avocado\n",
    "          ‘4225’              Total sales volume of Large Hass Avocado\n",
    "          ‘4770’              Total sales volume of Extra Large Hass Avocado\n",
    "          ‘Total Bags’        Total number of Bags sold\n",
    "          ‘Small Bags’        Total number of Small Bags sold\n",
    "          ‘Large Bags’        Total number of Large Bags sold\n",
    "          ‘XLarge Bags’       Total number of XLarge Bags sold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88c2413",
   "metadata": {},
   "source": [
    "# * Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4ede6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\", warn=False)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas_profiling\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected=True)\n",
    "from plotly import tools\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93915f1a",
   "metadata": {},
   "source": [
    "Read in the Avocado Prices csv file as a DataFrame called df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f32fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"https://raw.githubusercontent.com/insaid2018/Term-2/master/Projects/avocado.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9bada1",
   "metadata": {},
   "source": [
    "# * Data Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90424b7",
   "metadata": {},
   "source": [
    "# * Understanding the Avocado Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe94014",
   "metadata": {},
   "source": [
    "Lets check our data shape:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eacaab",
   "metadata": {},
   "source": [
    "# Dataset has 18249 rows and 14 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19ee2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cf62bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "(18249, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a197e38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns  # This will print the names of all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c9fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "Index(['Unnamed: 0', 'Date', 'AveragePrice', 'Total Volume', '4046', '4225',\n",
    "       '4770', 'Total Bags', 'Small Bags', 'Large Bags', 'XLarge Bags', 'type',\n",
    "       'year', 'region'],\n",
    "      dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ff60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()  # Will give you first 5 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b69e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    Unnamed:0   Date       AveragePric   Total Volume   4046     4225       4770   Total Bags   Small Bags   Large Bags  XLarge Bags   type            year   region\n",
    "    0          2015-12-27  1.33          64236.62       1036.7   54454.85   48.16  8696.87      8603.62      93.25       0.0           conventional    2015   Albany\n",
    "    1          2015-12-20  1.35          54876.98       674.28   44638.81   58.33  9505.56      9408.07      97.49       0.0           conventional    2015   Albany\n",
    "    2          2015-12-13  0.93          118220.22      794.70   109149.67  130.50 8145.35      8042.21      103.14      0.0           conventional    2015   Albany\n",
    "    3          2015-12-06  1.08          78992.15       1132.00  71976.41   72.58  5811.16      5677.40      133.76      0.0           conventional    2015   Albany\n",
    "    4          2015-11-29  1.28          51039.60       941.48   43838.39   75.78  6183.95      5986.26      197.69      0.0           conventional    2015   Albany"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ce080b",
   "metadata": {},
   "source": [
    "The Feature \"Unnamed:0\" is just a representation of the indexes, so it's useless to keep it, we'll remove it in pre-processing !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d59bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()  # This will print the last n rows of the Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792cf540",
   "metadata": {},
   "outputs": [],
   "source": [
    "         Unnamed: 0   Date        AveragePrice    Total Volume  4046     4225     4770    Total Bags   Small Bags    Large Bags   XLarge Bags   type     year   region\n",
    "18244    7            2018-02-04  1.63           17074.83       2046.96  1529.20  0.00    13498.67     13066.82     431.85        0.0           organic  2018   WestTexNewMexico\n",
    "18245    8            2018-01-28  1.71           13888.04       1191.70  3431.50  0.00    9264.84      8940.04      324.80        0.0           organic  2018   WestTexNewMexico\n",
    "18246    9            2018-01-21  1.87           13766.76       1191.92  2452.79  727.94  9394.11      9351.80      42.31         0.0           organic  2018   WestTexNewMexico\n",
    "18247    10           2018-01-14  1.93           16205.22       1527.63  2981.04  727.01  10969.54     10919.54     50.00         0.0           organic  2018   WestTexNewMexico\n",
    "18248    11           2018-01-07  1.62           17489.58       2894.77  2356.13  224.53  12014.15     11988.14     26.01         0.0           organic  2018   WestTexNewMexico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8b90d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() # This will give Index, Datatype and Memory information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee3ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 18249 entries, 0 to 18248\n",
    "Data columns (total 14 columns):\n",
    "Unnamed: 0      18249 non-null int64\n",
    "Date            18249 non-null object\n",
    "AveragePrice    18249 non-null float64\n",
    "Total Volume    18249 non-null float64\n",
    "4046            18249 non-null float64\n",
    "4225            18249 non-null float64\n",
    "4770            18249 non-null float64\n",
    "Total Bags      18249 non-null float64\n",
    "Small Bags      18249 non-null float64\n",
    "Large Bags      18249 non-null float64\n",
    "XLarge Bags     18249 non-null float64\n",
    "type            18249 non-null object\n",
    "year            18249 non-null int64\n",
    "region          18249 non-null object\n",
    "dtypes: float64(9), int64(2), object(3)\n",
    "memory usage: 1.9+ MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaf3e23",
   "metadata": {},
   "source": [
    "Well as a first observation we can see that we are lucky, we dont have any missing values (18249 complete data) and 13 columns. Now let's do some Feature Engineering on the Date Feature in pre-processing later so we can be able to use the day and the month columns in building our machine learning model later. ( I didn't mention the year because its already there in data frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b2ee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use include='all' option to generate descriptive statistics for all columns\n",
    "# You can get idea about which column has missing values using this\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ea1868",
   "metadata": {},
   "outputs": [],
   "source": [
    "        Unnamed: 0     AveragePrice   Total Volume   4046           4225           4770           Total Bags      Small Bags   Large Bags      XLarge Bags    Year\n",
    "count   18249.000000   18249.000000   1.824900e+04   1.824900e+04   1.824900e+04   1.824900e+04   1.824900e+04   1.824900e+04  1.824900e+04   18249.000000    18249.00000\n",
    "mean    24.232232      1.405978       8.506440e+05   2.930084e+05   2.951546e+05   2.283974e+04   2.396392e+05   1.821947e+05  5.433809e+04   3106.426507     2016.147899\n",
    "std     15.481045      0.402677       3.453545e+06   1.264989e+06   1.204120e+06   1.074641e+05   9.862424e+05   7.461785e+05  2.439660e+05   17692.894659    0.939938\n",
    "min     0.000000       0.440000       8.456000e+01   0.000000e+00   0.000000e+00   0.000000e+00   0.000000e+00   0.000000e+00  0.000000e+00   0.000000        2015.000000\n",
    "25%     10.000000      1.100000       1.083858e+04   8.540700e+02   3.008780e+03   0.000000e+00   5.088640e+03   2.849420e+03  1.274700e+02   0.000000        2015.000000\n",
    "50%     24.000000      1.370000       1.073768e+05   8.645300e+03   2.906102e+04   1.849900e+02   3.974383e+04   2.636282e+04  2.647710e+03   0.000000        2016.000000\n",
    "75%     38.000000      1.660000       4.329623e+05   1.110202e+05   1.502069e+05   6.243420e+03   1.107834e+05   8.333767e+04  2.202925e+04   132.500000      2017.000000\n",
    "max     52.000000      3.250000       6.250565e+07   2.274362e+07   2.047057e+07   2.546439e+06   1.937313e+07   1.338459e+07  5.719097e+06   551693.650000   2018.000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd28873d",
   "metadata": {},
   "source": [
    "We can see all columns having count 18249. Looks like it doesn't contain missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c6ffc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()  # Will show you null count for each column, but will not count Zeros(0) as null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9920eb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Unnamed: 0      0\n",
    "Date            0\n",
    "AveragePrice    0\n",
    "Total Volume    0\n",
    "4046            0\n",
    "4225            0\n",
    "4770            0\n",
    "Total Bags      0\n",
    "Small Bags      0\n",
    "Large Bags      0\n",
    "XLarge Bags     0\n",
    "type            0\n",
    "year            0\n",
    "region          0\n",
    "dtype: int64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc44a4e",
   "metadata": {},
   "source": [
    "We can see that no missing values exist in dataset, that's great!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a786e787",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6dd5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = pandas_profiling.ProfileReport(df)\n",
    "profile.to_file(outputfile=\"avocado_before_preprocessing.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae878423",
   "metadata": {},
   "source": [
    ".I have done Pandas Profiling before preprocessing dataset, so we can get initial observations from the dataset in better visual   aspects, to find correlation matrix and sample data. File was saved as html file avocado_before_preprocessing.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d6f004",
   "metadata": {},
   "source": [
    ".Will take a look at the file and see what useful insight you can develop from it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47a12d8",
   "metadata": {},
   "source": [
    "Initial observation as a result from profiling of Avocado Dataset can be seen in avocado_before_preprocessing.html"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6eb5220e",
   "metadata": {},
   "source": [
    "dataset info                                     Variables types\n",
    "   Number of variables             14             Numeric          6\n",
    "   Number of observations          18249          Categorical      3\n",
    "   Total Missing (%)               0.0%           Boolean          0\n",
    "   Totla size in memory            1.9 MiB        date             0\n",
    "   Average record size in memory   112.0 B        Text (Unique)    0\n",
    "                                                  Rejection        5\n",
    "                                                  Unsupported      0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143bb6fb",
   "metadata": {},
   "source": [
    "# Warnings"
   ]
  },
  {
   "cell_type": "raw",
   "id": "20f37333",
   "metadata": {},
   "source": [
    "4046  is highly correlated with  Total Volume (p= 0.97786)   Rejected\n",
    "4225  is highly correlated with  4046         (p = 0.92611)  Rejected\n",
    "4770  has 5497 / 30.1% zeros  Zetos\n",
    "Date  has a high cardinality: 169 distint values  Warning\n",
    "Lare Bags  is highly correlated with small bags (p=0.90259) Rejecting\n",
    "small bags is highly correlated with total bags (p=0.99433) Rejecting\n",
    "Total bags is highly correlated with 4225 (p=0.90579)       Rejecting\n",
    "Unnamed: 0 has 432 / 2.4% zeros Zeros\n",
    "XL arhe Bags has 12048 / 66.0% zeros Zeros\n",
    "region has a high cardinality . 54 distinct values Warning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b305caaf",
   "metadata": {},
   "source": [
    "# * Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efb5599",
   "metadata": {},
   "source": [
    "The Feature \"Unnamed:0\" is just a representation of the indexes, so it's useless to keep it, lets remove it now !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef23f712",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Unnamed: 0',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bfc68b",
   "metadata": {},
   "source": [
    "Lets check our data head again to make sure that the Feature Unnamed:0 is removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226bea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9172fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "     Date        AveragePrice  Total Volume   4046     4225       4770    Total Bags  Small Bags  Large Bags  XLarge Bags  type          year  region\n",
    "0   2015-12-27   1.33          64236.62       1036.74  54454.85   48.16   8696.87     8603.62     93.25       0.0          conventional  2015  Albany\n",
    "1   2015-12-20   1.35          54876.98       674.28   44638.81   58.33   9505.56     9408.07     97.49       0.0          conventional  2015  Albany\n",
    "2   2015-12-13   0.93          118220.22      794.70   109149.67  130.50  8145.35     8042.21     103.14      0.0          conventional  2015  Albany\n",
    "3   2015-12-06   1.08          78992.15       1132.00  71976.41   72.58   5811.16     5677.40     133.76      0.0          conventional  2015  Albany\n",
    "4   2015-11-29   1.28          51039.60       941.48   43838.39   75.78   6183.95     5986.26     197.69      0.0          conventional  2015  Albany"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c2ec68",
   "metadata": {},
   "source": [
    "Earlier in info we have seen that Date is Object type not the date type. We have to change its type to date type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80328cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date']=pd.to_datetime(df['Date'])\n",
    "df['Month']=df['Date'].apply(lambda x:x.month)\n",
    "df['Day']=df['Date'].apply(lambda x:x.day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a743f935",
   "metadata": {},
   "source": [
    "Lets check the head to see what we have done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cc0ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403426d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    Date         AveragePrice  Total Volume  4046     4225      4770   Total Bags  Small Bags  Large Bags   XLarge Bags   type          year  region   Month  Day\n",
    "0   2015-12-27   1.33          64236.62      1036.74  54454.85  48.16   8696.87    8603.62     93.25        0.0           conventional  2015  Albany   12     27\n",
    "1   2015-12-20   1.35          54876.98      674.28   44638.81  58.33   9505.56    9408.07     97.49        0.0           conventional  2015  Albany   12     20\n",
    "2   2015-12-13   0.93          118220.22     794.70   109149.67 130.50  8145.35    8042.21     103.14       0.0           conventional  2015  Albany   12     13\n",
    "3   2015-12-06   1.08          78992.15      1132.00  71976.41  72.58   5811.16    5677.40     133.76       0.0           conventional  2015  Albany   12     6\n",
    "4   2015-11-29   1.28          51039.60      941.48   43838.39  75.78   6183.95    5986.26     197.69       0.0           conventional  2015  Albany   11     29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cf9ed1",
   "metadata": {},
   "source": [
    "# Data Visualisation and Questions answered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf616a1",
   "metadata": {},
   "source": [
    "*Organic vs Conventional** : The main difference between organic and conventional food products are the chemicals involved during production and processing. The interest in organic food products has been rising steadily over the recent years with new health super fruits emerging. Let's see if this is also the case with our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925b7046",
   "metadata": {},
   "source": [
    "# Q.1 Which type of Avocados are more in demand (Conventional or Organic)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc105f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "Type=df.groupby('type')['Total Volume'].agg('sum')\n",
    "\n",
    "values=[Type['conventional'],Type['organic']]\n",
    "labels=['conventional','organic']\n",
    "\n",
    "trace=go.Pie(labels=labels,values=values)\n",
    "py.iplot([trace])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815e4b1b",
   "metadata": {},
   "source": [
    "Just over 2% of our dataset is organic. So looks like Conventional is in more demand. Now, let's look at the average price distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70c70e7",
   "metadata": {},
   "source": [
    "# Q.2 In which range Average price lies, what is distribution look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd4ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5) \n",
    "from scipy.stats import norm\n",
    "fig, ax = plt.subplots(figsize=(15, 9))\n",
    "sns.distplot(a=df.AveragePrice, kde=False, fit=norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2082a62",
   "metadata": {},
   "source": [
    "\n",
    "It seems like you're trying to create a distribution plot using Seaborn and SciPy. However, it appears that you haven't imported the necessary libraries, such as Seaborn and Matplotlib. Also, the variable df.AveragePrice is referenced without defining the dataframe df. You need to ensure you have imported your data and assigned it to df before running this code.\n",
    "\n",
    "Here's a modified version of your code that includes the necessary imports and assumes you have a dataframe named df with a column named AveragePrice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f7aef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Assuming you have a dataframe named df with a column named AveragePrice\n",
    "sns.set(font_scale=1.5) \n",
    "fig, ax = plt.subplots(figsize=(15, 9))\n",
    "sns.distplot(a=df['AveragePrice'], kde=False, fit=norm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db56b26",
   "metadata": {},
   "source": [
    "Average Price distribution shows that for most cases price of avocado is between 1.1, 1.4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7102fad8",
   "metadata": {},
   "source": [
    "Let's look at average price of conventional vs. organic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bf1c20",
   "metadata": {},
   "source": [
    "# Q.3 How Average price is distributed over the months for Conventional and Organic Types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36697c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,10))\n",
    "sns.lineplot(x=\"Month\", y=\"AveragePrice\", hue='type', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db244a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(18, 10))\n",
    "sns.lineplot(x=\"Month\", y=\"AveragePrice\", hue='type', data=df)\n",
    "plt.title('Average Price Over Months by Type')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2db0157",
   "metadata": {},
   "source": [
    "Looks like there was a hike between months 8 – 10 for both Conventional and Organic type of Avocados prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725a878a",
   "metadata": {},
   "source": [
    "# Now lets plot Average price distribution based on region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c406898a",
   "metadata": {},
   "source": [
    "# Q.4 What are TOP 5 regions where Average price are very high?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbf7a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_list=list(df.region.unique())\n",
    "average_price=[]\n",
    "\n",
    "for i in region_list:\n",
    "    x=df[df.region==i]\n",
    "    region_average=sum(x.AveragePrice)/len(x)\n",
    "    average_price.append(region_average)\n",
    "\n",
    "df1=pd.DataFrame({'region_list':region_list,'average_price':average_price})\n",
    "new_index=df1.average_price.sort_values(ascending=False).index.values\n",
    "sorted_data=df1.reindex(new_index)\n",
    "\n",
    "plt.figure(figsize=(24,10))\n",
    "ax=sns.barplot(x=sorted_data.region_list,y=sorted_data.average_price)\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Average Price')\n",
    "plt.title('Average Price of Avocado According to Region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf6e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "region_list = list(df.region.unique())\n",
    "average_price = []\n",
    "\n",
    "for region in region_list:\n",
    "    region_data = df[df.region == region]\n",
    "    region_average = region_data.AveragePrice.mean()\n",
    "    average_price.append(region_average)\n",
    "\n",
    "df1 = pd.DataFrame({'region_list': region_list, 'average_price': average_price})\n",
    "sorted_data = df1.sort_values(by='average_price', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(24, 10))\n",
    "ax = sns.barplot(x='average_price', y='region_list', data=sorted_data, palette='viridis')\n",
    "\n",
    "plt.xlabel('Average Price')\n",
    "plt.ylabel('Region')\n",
    "plt.title('Average Price of Avocado According to Region')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4396e869",
   "metadata": {},
   "source": [
    ".  Looks like these region are where price is very high\n",
    "    .  HartfordSpringfield\n",
    "    .  SanFrancisco\n",
    "    .  NewYork\n",
    "    .  Philadelphia\n",
    "    .  Sacramento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b7a1d4",
   "metadata": {},
   "source": [
    "# Q.5 What are TOP 5 regions where Average consumption is very high?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b19b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter1=df.region!='TotalUS'\n",
    "df1=df[filter1]\n",
    "\n",
    "region_list=list(df1.region.unique())\n",
    "average_total_volume=[]\n",
    "\n",
    "for i in region_list:\n",
    "    x=df1[df1.region==i]\n",
    "    average_total_volume.append(sum(x['Total Volume'])/len(x))\n",
    "df3=pd.DataFrame({'region_list':region_list,'average_total_volume':average_total_volume})\n",
    "\n",
    "new_index=df3.average_total_volume.sort_values(ascending=False).index.values\n",
    "sorted_data1=df3.reindex(new_index)\n",
    "\n",
    "plt.figure(figsize=(22,10))\n",
    "ax=sns.barplot(x=sorted_data1.region_list,y=sorted_data1.average_total_volume)\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Average of Total Volume')\n",
    "plt.title('Average of Total Volume According to Region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d15b9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "filter1 = df.region != 'TotalUS'\n",
    "df1 = df[filter1]\n",
    "\n",
    "region_list = list(df1.region.unique())\n",
    "average_total_volume = []\n",
    "\n",
    "for region in region_list:\n",
    "    region_data = df1[df1.region == region]\n",
    "    region_average = region_data['Total Volume'].mean()\n",
    "    average_total_volume.append(region_average)\n",
    "\n",
    "df3 = pd.DataFrame({'region_list': region_list, 'average_total_volume': average_total_volume})\n",
    "\n",
    "plt.figure(figsize=(22, 10))\n",
    "ax = sns.barplot(x='region_list', y='average_total_volume', data=df3, order=df3.sort_values('average_total_volume', ascending=False).region_list)\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Average of Total Volume')\n",
    "plt.title('Average of Total Volume According to Region')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b989b5e3",
   "metadata": {},
   "source": [
    "This code will generate a bar plot showing the average total volume of avocado sales for each region, sorted in descending order. Adjust the figure size, axis labels, and title as needed for your visualization."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e6dff9d",
   "metadata": {},
   "source": [
    ".   Looks like these region are where Consumption is very high\n",
    "       .  West\n",
    "       .  California\n",
    "       .  SouthCentral\n",
    "       .  Northeast\n",
    "       .  Southeast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613a2c8a",
   "metadata": {},
   "source": [
    "# Q.6 In which year and for which region was the Average price the highest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9f2153",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.factorplot('AveragePrice','region',data=df,\n",
    "                   hue='year',\n",
    "                   size=18,\n",
    "                   aspect=0.7,\n",
    "                   palette='Blues',\n",
    "                   join=False,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f4e9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "g = sns.catplot(x='AveragePrice', y='region', data=df,\n",
    "                   hue='year',\n",
    "                   height=18,\n",
    "                   aspect=0.7,\n",
    "                   palette='Blues',\n",
    "                   kind='strip',  # 'strip' plot will show individual data points\n",
    "                   jitter=False  # Set jitter to False to align points on the categorical axis\n",
    "              )\n",
    "g.set_xticklabels(rotation=90)  # Rotate x-axis labels for better readability\n",
    "g.set_axis_labels('Average Price', 'Region')  # Set axis labels\n",
    "g.fig.suptitle('Average Price of Avocados by Region and Year')  # Set title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07e0891",
   "metadata": {},
   "source": [
    "This will create a categorical scatter plot (strip plot) where each point represents the average price of avocados for a specific region and year. The points will be colored based on the year. Adjust the height, aspect, palette, and other parameters as needed for your visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87479bcc",
   "metadata": {},
   "source": [
    "Looks like there was a huge increase in Avocado prices as the demand was little high in Year 2017 in SanFranciso region. If you'll search it on google, you'll find the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7fc46",
   "metadata": {},
   "source": [
    "# Q.7 How price is distributed over the date column?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb1d97",
   "metadata": {},
   "source": [
    "Now lets do some plots!! I'll start by plotting the Avocado's Average Price through the Date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baa6984",
   "metadata": {},
   "outputs": [],
   "source": [
    "byDate=df.groupby('Date').mean()\n",
    "plt.figure(figsize=(12,8))\n",
    "byDate['AveragePrice'].plot()\n",
    "plt.title('Average Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a08891e",
   "metadata": {},
   "source": [
    "It appears you're aggregating your data by date, calculating the mean of the 'AveragePrice' column for each date, and then plotting the average price over time. Here's how you can do that using Matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447647f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a DataFrame named df with a 'Date' column and an 'AveragePrice' column\n",
    "byDate = df.groupby('Date').mean()\n",
    "plt.figure(figsize=(12, 8))\n",
    "byDate['AveragePrice'].plot()\n",
    "plt.title('Average Price Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb3da03",
   "metadata": {},
   "source": [
    "This will generate a line plot showing how the average price changes over time. Adjust the title, x-axis label, and y-axis label as needed for your visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7ab90e",
   "metadata": {},
   "source": [
    ". This also shows there was a huge hike in prices after July 2017 and before Jan 2018. This was also confirmed in earlier graph    too.\n",
    ". Cool right? now lets have an idea about the relationship between our Features(Correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c0d084",
   "metadata": {},
   "source": [
    "# Q.8 How dataset features are correlated with each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280141d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(df.corr(),cmap='coolwarm',annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b779da",
   "metadata": {},
   "source": [
    "Your code intends to create a heatmap using Seaborn to visualize the correlation between different numerical variables in your DataFrame. It's a great way to quickly identify relationships between variables. Here's the corrected version of your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e7a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(df.corr(), cmap='coolwarm', annot=True)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649c11e3",
   "metadata": {},
   "source": [
    "As we can from the heatmap above, all the Features are not correleted with the Average Price column, instead most of them are correlated with each other. So now I am bit worried because that will not help us get a good model. Lets try and see.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fd9be0",
   "metadata": {},
   "source": [
    "First we have to do some Feature Engineering on the categorical Features : region and type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95b1d04",
   "metadata": {},
   "source": [
    "# * Feature Engineering for Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b71a86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['region'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee34d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc4daa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['type'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9719becb",
   "metadata": {},
   "outputs": [],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c00892",
   "metadata": {},
   "source": [
    "As we can see we have 54 regions and 2 unique types, so it's going to be easy to to transform the type feature to dummies, but for the region its going to be a bit complex, so I decided to drop the entire column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d70690",
   "metadata": {},
   "source": [
    "I will drop the Date Feature as well because I already have 3 other columns for the Year, Month and Day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9d74ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=pd.get_dummies(df.drop(['region','Date'],axis=1),drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1eb337",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e2295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "   veragePrice  Total Volume  4046     4225       4770    Total Bags  Small Bags   Large Bags   XLarge Bags  year   Month   Day   type_organic\n",
    "0  1.33         64236.62      1036.74  54454.85   48.16   8696.87     8603.62      93.25        0.0          2015     12     27     0\n",
    "1  1.35         54876.98      674.28   44638.81   58.33   9505.56     9408.07      97.49        0.0          2015     12     27     0\n",
    "2  0.93         118220.22     794.70   109149.67  130.50  8145.35     8042.21      103.14       0.0          2015     12     13     0\n",
    "3  1.08         78992.15      1132.00  71976.41   72.58   5811.16     5677.40      133.76       0.0          2015     12     6      0\n",
    "4  1.28         51039.60      941.48   43838.39   75.78   6183.95     5986.26      197.69       0.0          2015     11     29     0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62cf448",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f386489c",
   "metadata": {},
   "outputs": [],
   "source": [
    "        AveragePrice  Total Volume  4046     4225     4770    Total Bags  Small Bags   Large Bags   XLarge Bags  year   Month  Day  type_organic\n",
    "18244   1.63          17074.83      2046.96  1529.20  0.00    13498.67    13066.82     431.85       0.0          2018   2      4    1\n",
    "18245   1.71          13888.04      1191.70  3431.50  0.00    9264.84     8940.04      324.80       0.0          2018   1      28   1\n",
    "18246   1.87          13766.76      1191.92  2452.79  727.94  9394.11     9351.80      42.31        0.0          2018   1      21   1\n",
    "18247   1.93          16205.22      1527.63  2981.04  727.01  10969.54    10919.54     50.00        0.0          2018   1      14   1\n",
    "18248   1.62          17489.58      2894.77  356.13   224.53  12014.15    11988.14     26.01        0.0          2018   1      7    1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980db969",
   "metadata": {},
   "source": [
    "# * Model selection/predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abd1382",
   "metadata": {},
   "source": [
    "Now our data are ready! lets apply our model which is going to be the Linear Regression because our Target variable 'AveragePrice' is continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a603dc0",
   "metadata": {},
   "source": [
    "Let's now begin to train out regression model! We will need to first split up our data into an X array that contains the features to train on, and a y array with the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49694202",
   "metadata": {},
   "source": [
    "# P.1 Are we good with Linear Regression? Lets find out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5642e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_final.iloc[:,1:14]\n",
    "y=df_final['AveragePrice']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f136bbd",
   "metadata": {},
   "source": [
    "Creating and Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e78012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr=LinearRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "pred=lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb16fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, pred))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "14e6f219",
   "metadata": {},
   "source": [
    "MAE: 0.23297133291665678\n",
    "MSE: 0.09108802805350158\n",
    "RMSE: 0.3018079323899582"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091a904e",
   "metadata": {},
   "source": [
    "The RMSE is low so we can say that we do have a good model, but lets check to be more sure.\n",
    "Lets plot the y_test vs the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da7416c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=y_test,y=pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf383423",
   "metadata": {},
   "source": [
    "As we can see that we don't have a straight line so I am not sure that this is the best model we can apply on our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200b1ebc",
   "metadata": {},
   "source": [
    "Lets try working with the Decision Tree Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fa5747",
   "metadata": {},
   "source": [
    "# P.2 Are we good with Decision Tree Regression? Lets find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977dc705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dtr=DecisionTreeRegressor()\n",
    "dtr.fit(X_train,y_train)\n",
    "pred=dtr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83c82f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=y_test,y=pred)\n",
    "plt.xlabel('Y Test')\n",
    "plt.ylabel('Predicted Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502368f2",
   "metadata": {},
   "source": [
    "It seems like you're trying to create a scatter plot to visualize the relationship between the actual values (y_test) and the predicted values (pred). This is a common approach for evaluating the performance of a regression model. Here's how you can create the scatter plot using Matplotlib:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf47b320",
   "metadata": {},
   "source": [
    "Nice, here we can see that we nearly have a straight line, in other words its better than the Linear regression model, and to be more sure lets check the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca910f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have y_test and pred arrays/lists containing the actual and predicted values respectively\n",
    "plt.scatter(x=y_test, y=pred)\n",
    "plt.xlabel('Y Test')\n",
    "plt.ylabel('Predicted Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9f7069",
   "metadata": {},
   "source": [
    "This will plot the actual values on the x-axis and the predicted values on the y-axis, allowing you to visually assess how closely they align. If the model predictions are accurate, the points should fall along a diagonal line from the bottom left to the top right of the plot.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702bafbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE:', metrics.mean_absolute_error(y_test, pred))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34d748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE: 0.13404109589041097\n",
    "MSE: 0.04273295890410959\n",
    "RMSE: 0.2067195174726121"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4cb71e",
   "metadata": {},
   "source": [
    "Very Nice, our RMSE is lower than the previous one we got with Linear Regression. Now I am going to try one last model to see if I can improve my predictions for this data which is the RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa16668",
   "metadata": {},
   "source": [
    "# P.3 Are we good with Random Forest Regressor? Lets find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23773db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rdr = RandomForestRegressor()\n",
    "rdr.fit(X_train,y_train)\n",
    "pred=rdr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4af8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning:\n",
    "\n",
    "numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2c90c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE:', metrics.mean_absolute_error(y_test, pred))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "deafc966",
   "metadata": {},
   "source": [
    "MAE: 0.10715068493150685\n",
    "MSE: 0.023817860821917808\n",
    "RMSE: 0.15433036260541153"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a46583",
   "metadata": {},
   "source": [
    "Well as we can see the RMSE is lower than the two previous models, so the RandomForest Regressor is the best model in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c779d991",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot((y_test-pred),bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f5d5be",
   "metadata": {},
   "source": [
    "Notice here that our residuals looked to be normally distributed and that's really a good sign which means that our model was a correct choice for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5187b681",
   "metadata": {},
   "source": [
    "# Lets see final Actual Vs Predicted sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95cef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'Y Test':y_test , 'Pred':pred},columns=['Y Test','Pred'])\n",
    "sns.lmplot(x='Y Test',y='Pred',data=data,palette='rainbow')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9705dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "      Y Test   Pred\n",
    "8604   0.82    0.993\n",
    "2608   0.97    0.998\n",
    "14581  1.44    1.344\n",
    "4254   0.97    0.894\n",
    "16588  1.45    1.451"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb418654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06d41e2d",
   "metadata": {},
   "source": [
    "# * Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e101c768",
   "metadata": {},
   "source": [
    "With the help of notebook I learnt how EDA can be carried out using Pandas and other plotting libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3202b5",
   "metadata": {},
   "source": [
    "Also I have seen making use of packages like matplotlib, plotly and seaborn to develop better insights about the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ace5ad7",
   "metadata": {},
   "source": [
    "I have also seen how preproceesing helps in dealing with missing values and irregualities present in the data. I also learnt how to create new features which will in turn help us to better predict the survival."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4631304",
   "metadata": {},
   "source": [
    "I also make use of pandas profiling feature to generate an html report containing all the information of the various features present in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73fceff",
   "metadata": {},
   "source": [
    "I have seen the impact of columns like type, year/date on the Average price increase/decrease rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09c1d6b",
   "metadata": {},
   "source": [
    "The most important inference drawn from all this analysis is, I get to know what are the features on which price is highly positively and negatively coorelated with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730dd845",
   "metadata": {},
   "source": [
    "I came to know through analysis which model will be work with better accuracy with the help of low residual and RMSE scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6114f8",
   "metadata": {},
   "source": [
    "This project helped me to gain insights and how I should go with flow, which model to choose first and go step by step to attain results with good accuracy. Also get to know where to use Linear, Decision Tree and other applicable and required models to fine tune the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba47253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
